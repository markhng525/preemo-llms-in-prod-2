{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data via Prompt Engineering\n",
    "This notebook demonstrates how to generate synthetic data via prompt engineering (few-shot prompts). The exemplars are taken from the MathGSM8K dataset and we will be doing inference on the Flan-UL2 model. \n",
    "The purpose is to show the efficacy and limitations of a simple data generation method and how to set up a prompt-engineering pipeline for data synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from constants import MATH_DATASET_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Prompts (Few-Shot)\n",
    "First step is to upload and process the data for few-shot prompting. Note that the precise format, syntax, and semantics is critical for prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and preprocess data\n",
    "import pprint\n",
    "\n",
    "with open(MATH_DATASET_DIR + \"/math_questions_and_answers.jsonl\") as f:\n",
    "    data = [json.loads(line)['sample'] for line in f]\n",
    "\n",
    "pprint.pprint(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# random sampling of exemplars, can select via more elaborate methods\n",
    "def sample_exemplars(sampling_pool, k):\n",
    "    return random.sample(sampling_pool, k=k)\n",
    "\n",
    "# choose 8 exemplars\n",
    "list_of_exemplars = sample_exemplars(data, 8)\n",
    "\n",
    "pprint.pprint(list_of_exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose question from dataset of unanswered questions\n",
    "with open(MATH_DATASET_DIR + \"/math_questions.jsonl\") as f:\n",
    "    problems = [json.loads(line)['sample'] for line in f]\n",
    "problem = sample_exemplars(problems, 1)\n",
    "\n",
    "# format prompt: metaprompt, exemplars, and template\n",
    "metaprompt = \"Solve the math problem step-by-step.\"\n",
    "exemplars = \"\\n\".join(list_of_exemplars)\n",
    "template = problem[0][:-1] + \"\\nAnswer: \"\n",
    "\n",
    "prompt_string = metaprompt + \"\\n\" + exemplars + \"\\n\" + template\n",
    "\n",
    "print(prompt_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference\n",
    "Second step is to make an inference call with the generated prompt to get some completions. Here we will make an inference call to Flan-UL2 model with the prompt we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import LOCAL_MODELS_ROOT, HF_MODELS_ROOT\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"flan-ul2\"\n",
    "\n",
    "model_path_or_id = (\n",
    "    f\"{LOCAL_MODELS_ROOT}/{BASE_MODEL}\"\n",
    "    if Path(f\"{LOCAL_MODELS_ROOT}/{BASE_MODEL}\").exists()\n",
    "    else f\"{HF_MODELS_ROOT}/{BASE_MODEL}\"\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_path_or_id\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_path_or_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "generation_config = transformers.GenerationConfig(\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    min_new_tokens=32,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_string, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "output = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Consistency\n",
    "Third step is to post-process the responses. Since we are doing a QA task on math questions, an effective post-processing step is a majority-scoring technique known as Self-Consistency. While it doesn't verify that the answer is correct, it gives a strong indication that it should be (as long as the model is sufficiently unbiased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Mark is three times older than Forrest. Forrest is two times older than Chris. If Chris is 4, how old is Mark?\n",
    "\n",
    "list_of_completions = [\n",
    "    \"Answer: Forrest is 4*2=8 years old. Mark is 8*3=24 years old.\\nThe answer is 24.\",\n",
    "    \"Answer: Forrest is twice Chris's age, which is 4, so Forrest is 4*2=8. Mark is thrice Forrest's age, which is 8, so Mark is 8*3=24.\\nThe answer is 24.\",\n",
    "    \"Answer: Chris is 4, so Forrest is 2*4=10. Mark is 3*10=34.\\nThe answer is 34.\",\n",
    "    \"Answer: Chris is 4. Forrest is 4*2=8. Mark is 8*3=24.\\nThe answer is 24.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# process the list of completions for validation\n",
    "def create_validation_set(sentences: list[str]):\n",
    "    validation_set = defaultdict(list)\n",
    "    pattern = \"\\nThe answer is \"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        start_idx = sentence.rfind(pattern)\n",
    "        if start_idx != -1:\n",
    "            substring = sentence[start_idx + len(pattern) :].strip()\n",
    "            try:\n",
    "                number = re.findall(r\"\\d+\\.?\\d*\", substring)\n",
    "                validation_set[float(number[0])].append(sentence)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "    return validation_set\n",
    "\n",
    "# create validation set from list of completions\n",
    "validation_set = create_validation_set(list_of_completions)\n",
    "\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority scoring for determining the most consistent of answers\n",
    "def self_consistency(validation_set: dict[float,list]):\n",
    "    if len(validation_set.keys()) > 0:\n",
    "        most_frequent_key = max(validation_set, key=lambda k: len(validation_set[k]))\n",
    "        return validation_set[most_frequent_key]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "filtered_completions = self_consistency(validation_set)\n",
    "\n",
    "filtered_completions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "The final step is evaluation. For this demo we want diverse responses so that we increase the complexity of the dataset and make sure that we have a sufficiently broad set of reasoning paths in our dataset. We use the Rouge-L metric as a proxy to assess diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    # candidate, reference: generated and ground-truth sentences\n",
    "    scores = rouge.get_scores(candidate, reference)[0]['rouge-l']['r']\n",
    "    return scores\n",
    "\n",
    "candidate = \"hi, everyone, it's nice to meet you!\"\n",
    "reference = \"hi, it's nice to meet everyone!\"\n",
    "\n",
    "# longest subsequences are \"hi\" and \"it's nice to meet\"\n",
    "# 5 words out of 6 in the reference, 5/6 ~ 0.8333333\n",
    "# 5 words out of 7 in the candidate, 5/7 ~ 0.7142857\n",
    "\n",
    "print(calculate_rouge(candidate, reference))\n",
    "print(calculate_rouge(reference, candidate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# returns rouge score over pairwise scoring against a list\n",
    "def get_rouge_score_per_sentence(sentence: str, remaining_sentences: list[str]):\n",
    "    rouge_scores = []\n",
    "    for remaining_sentence in remaining_sentences:\n",
    "        rouge_score = calculate_rouge(sentence, remaining_sentence)\n",
    "        rouge_scores.append(rouge_score)\n",
    "    return rouge_scores\n",
    "\n",
    "\n",
    "# returns average rouge score over pairwise scoring\n",
    "def calculate_self_rouge(sentences: list[str]):\n",
    "    rouge_scores = []\n",
    "\t\n",
    "    for sentence in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        sentences_copy.remove(sentence)\n",
    "        rouge_score = get_rouge_score_per_sentence(sentence,sentences_copy)\n",
    "        rouge_scores.append(rouge_score)\n",
    "\n",
    "    average_rouge_scores_per_sentence = [\n",
    "        np.mean(sentence_rouge_scores) for sentence_rouge_scores in rouge_scores\n",
    "        ]\n",
    "    average_rouge_score = np.mean(average_rouge_scores_per_sentence)\n",
    "\n",
    "    return average_rouge_scores_per_sentence, average_rouge_score\n",
    "\n",
    "calculate_self_rouge([candidate,reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator discards rouge scores that are too high\n",
    "def rouge_evaluator(sentences):\n",
    "    rouge_scores, average_rouge_score = calculate_self_rouge(sentences)\n",
    "    evaluated_sentences = []\n",
    "\n",
    "    # remove any sentence whose average rouge score is too high\n",
    "    for i in range(len(rouge_scores)):\n",
    "        if rouge_scores[i] < average_rouge_score:\n",
    "            evaluated_sentences.append(sentences[i])\n",
    "\n",
    "    return evaluated_sentences\n",
    "\n",
    "rouge_evaluator([candidate, reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the filtered completions\n",
    "print(calculate_self_rouge(filtered_completions))\n",
    "\n",
    "rouge_evaluator(filtered_completions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
